---
title: 【月报】2025年12月
date: 2025.12.19
updated:
tags:
  - 月报
categories: 时流
---

## 输入

- [A Series of Vignettes From My Childhood and Early Career](https://www.jasonscheirer.com/weblog/vignettes/)

- [Announcing the NeurIPS 2025 Best Paper Awards](https://blog.neurips.cc/2025/11/26/announcing-the-neurips-2025-best-paper-awards/)

  - [Artificial Hivemind: The Open-Ended Homogeneity of Language Models (and Beyond)](https://openreview.net/forum?id=1b7whO4SfY)

    > 本文通过引入 **Infinity-Chat** 大规模数据集，揭示了大型语言模型（LMs）在回答开放式查询时存在严重的“**人工蜂巢思维**”效应，表现为模型内部和不同模型之间生成内容的高度**同质化**，并指出这可能对人类创造力和思想多元化构成长期风险。

  - [Gated Attention for Large Language Models: Non-linearity, Sparsity, and Attention-Sink-Free](https://openreview.net/forum?id=1b7whO4SfY)

    > 该研究发现了一个简单的修改——在缩放点积注意力（SDPA）后添加**特定于注意力头部的 Sigmoid 门控**——能够持续提升大型语言模型的性能，提高训练稳定性，并有效缓解“注意力汇聚”现象和增强长上下文外推能力。

  - [1000 Layer Networks for Self-Supervised RL: Scaling Depth Can Enable New Goal-Reaching Capabilities](https://openreview.net/forum?id=s0JVsx3bx1)

    > 该研究挑战了强化学习（RL）应使用浅层网络的惯例，证明在**自监督目标导向的强化学习**中，将网络深度大幅扩展至 **1024 层**可以显著提升性能，并解锁代理的全新目标达成能力。

  - [Why Diffusion Models Don’t Memorize: The Role of Implicit Dynamical Regularization in Training](https://openreview.net/forum?id=BSZqpqgqM0)

    > 该研究通过理论和实验证明扩散模型在训练中具有**隐式动力学正则化**特性，表现为有效泛化窗口 $t_m$ 的长度随训练数据量 $N$ **线性增长**，从而**推迟了记忆（过拟合）的出现**，解释了扩散模型为何能在高度超参数化下仍能有效泛化。

  - ……
  
- [再读《马斯克传》，决定命运的细节都藏在他二十岁以前](https://mp.weixin.qq.com/s/iEZMaBV4Oab5fCsDBJMZKQ)

  - 入口够早，轨迹就会提前几年，而这几年会在未来放大成几十年的差距。
  - 很多人以为自己起步晚，所以自动放弃；真正聪明的人，会想办法把入口往前提前一点。资源不够，可以用时间补；没设备，可以从开源开始；没导师，可以自己去敲门。
  - 你第一次真正触到世界边界是什么时候，往往比你触到了什么更重要。入口不需要天生给你，你可以自己造。真正的差距，就是从你愿不愿意提前一步开始被拉开的。
  - 强者不会主动来找你，世界只能靠你敲门。
  - 极高密度输入；闭环做事；主动迁移环境；把痛苦当加速器；直接连接权力中心；提前思考二十年后的命题。
  - 学会 cold call 世界。写邮件、打电话、约一次面。只要成功一次，你对世界的理解就会被永久更新。

- [Six Big Bets - Jerry Liu](https://www.jerry.wtf/posts/six-big-bets/)

  - “有限下行风险，无限上行收益但。人们常忽略的是：**这些机会是有限的**。你不能无休止地挥棒。人生的大部分时间其实是在积累资源——知识、资本、人脉——从而为下一次挥棒做准备。
  - Age 18：ignorance is an asset.
  - Age 23：
    - first real exposure to how institutions work, usually through a job, or a failed first attempt
    - see the mismatch between incentives and outcomes
  - Age 28：
    - likely reached senior-to-staff level if you’re an IC, or middle management if you’re not. Your network is forming. You can recruit. You can sell credibility. And crucially, you can still grind.
    - the highest raw-energy + competence overlap of a career
  - Age 36:
    - risk tolerance is lower, this bet often starts as consulting, services, or a wedge product before productizing.
    - deep domain expertise and pattern recognition
  - Age 42:
    - bets become explicit and hypothesis-driven
    - You test assumptions one at a time. You hire ahead. You optimize for probability, not heroics
  - Age 51:
    - speed is no longer an advantage. Durability is.
    - Impact here also increasingly shifts toward ideas
  - Two mistakes people make:
    1. Not understanding what advantages you have. You need to lean into your energy, experience, or network as appropriate. Nobody believes the 36 year old founder is going to win on velocity alone against the new grads.
    2. Forgetting which phase you’re in. A lot of life is spent acquiring resources, and you don’t need to take on every single challenge if you’re not prepared. That dilutes the quality and success rate of your plays.

## 资源

- [Paged Out! 技术杂志](https://pagedout.institute/)

  Paged Out! 是一本免费的实验性技术杂志，专注于编程技巧、黑客技术、安全黑客、复古电脑、现代电脑、电子学、demoscene等话题。这本杂志由社区为社区制作，内容丰富且更新频繁。每一期杂志都充满了技术洞见和实用指南，吸引了大量读者下载和打印。最新一期《Paged Out!》第7期，主题为‘最佳README’，下载量已超过155,367次。

  > "这本杂志不仅是一本杂志，更是程序员们的技术宝典。"
  >
  > 可以找他们投稿
  
- [CS自学指南](https://csdiy.wiki/)

- 

## 产品/项目

- 
